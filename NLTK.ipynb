{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSI140b Lab 3: A Brief Tutorial on Machine Learning with NLTK/Python\n",
    "Keigh Rim, 4/12/2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This is a tutorial on machine learning techniques using NLTK in Python. It is based on Python 2.7 and NLTK3. \n",
    "\n",
    "The tutorial is about \n",
    "* transforming documents into feature representation\n",
    "* training existing machine learning algorithms in NLTK\n",
    "* testing the trained algorithms \n",
    "\n",
    "It is not about \n",
    "* engineering features\n",
    "* theoretical aspects of machine learning\n",
    "\n",
    "Now, let's start with importing nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification\n",
    "\n",
    "Here we will write Python code to train and test out a classifier using NLTK packages.\n",
    "\n",
    "Our example task would be training a Naive Bayes classifier that predicts gender of the author given a blog post. \n",
    "As our dataset, we will use [`blog-gender-dataset`](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). For details, refer to the paper;\n",
    "* Mukherjee, A., & Liu, B. (2010, October). Improving gender classification of blog authors. In Proceedings of the 2010 conference on Empirical Methods in natural Language Processing (pp. 207-217). Association for Computational Linguistics.\n",
    "\n",
    "In the dataset, we have about 3k blog posts, their raw unicode texts and their labels ('F', 'M') indicating the authors' gender. Let's take a peek at it. (The file is in CSV format.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Long time no see. Like always I was rewriting it from scratch a couple of times. But nevertheless it's still java and now it uses metropolis sampling to help that poor path tracing converge.\\n\\nBtw. I did MLT on yesterday evening after 2 beers (it had to be Ballmer peak).\\n\\nAltough the implementation is still very fresh it easily outperforms standard path tracing, what is to be seen especially when difficult caustics are involved.\\n\\nI've implemented spectral rendering too, it was very easy actually, cause all computations on wavelengths are linear just like rgb. But then I realised that even if it does feel more physically correct to do so, whats the point? 3d applications are operating in rgb color space, and because I cant represent a rgb color as spectrum interchangeably I have to approximate it, so as long as I'm not running a physical simulation or something I don't see the benefits (please correct me if I'm wrong), thus I abandoned that.\", 'M']\n",
      "[' Guest Demo: Eric Iverson\\xe2\\x80\\x99s Itty Bitty Search\\nFebruary 16th, 2010 by Daniel Tunkelang\\nRespond\\n\\nI\\xe2\\x80\\x99m back from vacation, and still digging my way out of everything that\\xe2\\x80\\x99s piled up while I\\xe2\\x80\\x99ve been offline.\\n\\nWhile I catch up, I thought I\\xe2\\x80\\x99d share with you a demo that Eric Iverson was gracious enough to share with me. It uses Yahoo! BOSS to support an exploratory search experience on top of a general web search engine.\\n\\nWhen you perform a query, the application retrieves a set of related term candidates using Yahoo\\xe2\\x80\\x99s key terms API. It then scores each term by dividing its occurrence count within the result set by its global occurrence count\\xe2\\x80\\x93a relevance measure similar to one my former colleagues and I used at Endeca in enterprise contexts.\\n\\nYou can try out the demo yourself at http://www.ittybittysearch.com/. While it has rough edges, it produces nice results\\xe2\\x80\\x93especially considering the simplicity of the approach.\\n\\nHere\\xe2\\x80\\x99s an example of how I used the application to explore and learn something new. I started with [\"information retrieval\"]. I noticed \\xe2\\x80\\x9cinteractive information retrieval\\xe2\\x80\\x9d as a top term, so I used it to refine. Most of the refinement suggestions looked familiar to me\\xe2\\x80\\x93but an unfamiliar name caught my attention: \\xe2\\x80\\x9cAnton Leuski\\xe2\\x80\\x9d. Following my curiosity, I refined again. Looking at the results, I immediately saw that Leuski had done work on evaluating document clustering for interactive information retrieval. Further exploration made it clear this is someone whose work I should get to know\\xe2\\x80\\x93check out his home page!\\n\\nI can\\xe2\\x80\\x99t promise that you\\xe2\\x80\\x99ll have as productive an experience as I did, but I encourage you to try Eric\\xe2\\x80\\x99s demo. It\\xe2\\x80\\x99s simple examples like these that remind me of the value of pursuing HCIR for the open web.\\n\\nSpeaking of which, HCIR 2010 is in the works. We\\xe2\\x80\\x99ll flesh out the details over the next weeks, and of course I\\xe2\\x80\\x99ll share them here.', 'M']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('blog-gender-dataset.csv') as dataset: \n",
    "    for i, row in enumerate(csv.reader(dataset)):\n",
    "        print(row)\n",
    "        if i > 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our *raw* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Representation\n",
    "\n",
    "So, as saw above, our *raw* dataset is simply a set of <**unicode string**, **label**> pairs.\n",
    "\n",
    "Now we are turning this into a <**features**, **label**> set. That is, we will write this piece of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dbceace8e9b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 for raw_text, label in csv.reader(dataset)}\n\u001b[1;32m     10\u001b[0m     feature_representation = {extract_features(text) : label \n\u001b[0;32m---> 11\u001b[0;31m                               \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                               if label} # some items are missing label\n",
      "\u001b[0;32m<ipython-input-7-dbceace8e9b9>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((text, label))\u001b[0m\n\u001b[1;32m     10\u001b[0m     feature_representation = {extract_features(text) : label \n\u001b[1;32m     11\u001b[0m                               \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                               if label} # some items are missing label\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-dbceace8e9b9>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blog-gender-dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def normalize_label(label):\n",
    "    return label.strip().upper()\n",
    "    \n",
    "def extract_features(text):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "with open('blog-gender-dataset.csv') as dataset: \n",
    "    raw_data = {raw_text : normalize_label(label) \n",
    "                for raw_text, label in csv.reader(dataset)}\n",
    "    feature_representation = {extract_features(text) : label \n",
    "                              for text, label in raw_data.iteritems() \n",
    "                              if label} # some items are missing label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining features\n",
    "Then, what is a feature? A feature is simply a fragmentary/single-faceted description of an instance/item in the data. It can be anything, for example: \n",
    "* *Does the document have a proper name in it? - **Yes***  (Binary feature)\n",
    "* *How many characters are appearing in the document? - **3***  (Discrete numerical feature)\n",
    "* *What is the most frequent non-stopword in the document? - **'time'*** (Nominal feature)\n",
    "\n",
    "A set of features is exactly the way we want to describe an item. And the set of feature values for an item is the representation of that item in the model we design. Thus, after all, we are transforming a document into a set of name-value pairs.\n",
    "\n",
    "However, to statistically model the data, each description (feature value) **needs to be a number**! Binary and numerical features are inherently convertible to numbers. Nominal or ordinal features can be mapped to numbers using random variables. Designing such random variables is beyond the scope of this tutorial and, for the sake of simplicity, we will be using only binary features in the rest of this section. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting feature values\n",
    "Next question: how can we get values of the features? For instance, say we have a binary feature that represents whether a document contains the word 'time'. How do we get the value for it? We write a feature function to get the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_time(document):\n",
    "    # returns a feature (yes | no) value directly\n",
    "    return 'time' in document.split()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides of the `has_time()` function, suppose we also have `has_flies()`, `has_like()`, `has_an()`, and `has_arrow()` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_flies(document):\n",
    "    return 'flies' in document.split()\n",
    "def has_like(document):\n",
    "    return 'like' in document.split()\n",
    "def has_an(document):\n",
    "    return 'an' in document.split()\n",
    "def has_arrow(document):\n",
    "    return 'arrow' in document.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then these two short documents are represented as follows in the feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document1:  {'has_an': True, 'has_like': True, 'has_arrow': True, 'has_time': True, 'has_flies': True}\n",
      "document2:  {'has_an': False, 'has_like': False, 'has_arrow': False, 'has_time': True, 'has_flies': True}\n"
     ]
    }
   ],
   "source": [
    "document1 = 'time flies like an arrow'\n",
    "document2 = 'time flies when you are having fun'\n",
    "\n",
    "doc1_features = {'has_time' : has_time(document1) ,\n",
    "                'has_flies' : has_flies(document1) ,\n",
    "                'has_like' : has_like(document1) ,\n",
    "                'has_an' : has_an(document1) ,\n",
    "                'has_arrow' : has_arrow(document1)}\n",
    "doc2_features = {'has_time' : has_time(document2) ,\n",
    "                'has_flies' : has_flies(document2) ,\n",
    "                'has_like' : has_like(document2) ,\n",
    "                'has_an' : has_an(document2) ,\n",
    "                'has_arrow' : has_arrow(document2)}\n",
    "print 'document1: ', doc1_features\n",
    "print 'document2: ', doc2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, our `extract_features()` functions can be written like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document1:  {'has_an': True, 'has_like': True, 'has_arrow': True, 'has_time': True, 'has_flies': True}\n"
     ]
    }
   ],
   "source": [
    "feature_functions = [has_time, \n",
    "                     has_flies, \n",
    "                     has_like, \n",
    "                     has_an, \n",
    "                     has_arrow ] \n",
    "\n",
    "def extract_features(document):\n",
    "    return {feature_function.__name__: feature_function(document) \n",
    "            for feature_function in feature_functions}\n",
    "\n",
    "print 'document1: ', extract_features(document1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that was a silly example, now let's consider a more realistic example using `blog-gender-dataset`. \n",
    "\n",
    "Let's say we want to use all the unigrams as our features, using simple whitespace tokenization. Then we are going to have ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128077\n"
     ]
    }
   ],
   "source": [
    "dataset_file = open('blog-gender-dataset.csv')\n",
    "raw_data = {raw_text: normalize_label(label) \n",
    "            for raw_text, label in csv.reader(dataset_file) if label}\n",
    "dataset_file.close()\n",
    "\n",
    "vocabulary = set()\n",
    "for document in raw_data.keys():\n",
    "    vocabulary.update(document.split())\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! We are going to have 128k features. So, do we need to write .1 million feature functions? Of course not. We can re-write the `extract_features()` function, to avoid writing 128k other functions, like such: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    # now each feature_function can handle multiple features\n",
    "    # and should return the values wrapped in a dict\n",
    "    features = {}\n",
    "    for feature_function in feature_functions:\n",
    "        features.update(feature_function(document))  \n",
    "    return features\n",
    "\n",
    "def unigram_bow(document):\n",
    "    return {token: (token in document.split()) for token in vocabulary}\n",
    "    \n",
    "feature_functions = [unigram_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how can you write another feature function `bigram_bow()` and incorporate it into `extract_features()`? And then how many features are we going to have? If we iterate through all three thousands of documents in the dataset and compute all the features, how long would it take? Can you write something in a more efficient way? For example, are all the word types including stopwords in the corpus going to be helpful features? How about stemming and lemmaization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature representation for NLTK classifiers\n",
    "Naive Bayes and MaxEnt in NLTK takes a list of tuples (feature_representation, label) as a training set. And they are smart enough to automatically take non-specified features as having *null*  values (`False`, `None`, `0`, etc) by default. As a result, for example, to get `unigram_bow()` features, we don't need to iterate over the whole vocabulary over and over. Rather, we re-write the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wavelengths': True, 'all': True, '(it': True, 'help': True, 'just': True, 'represent': True, 'color': True, 'scratch': True, 'too,': True, 'sampling': True, 'actually,': True, 'rendering': True, 'But': True, 'times.': True, 'see.': True, 'whats': True, 'Like': True, 'still': True, 'physical': True, 'feel': True, 'from': True, \"I've\": True, 'implementation': True, \"it's\": True, \"don't\": True, 'had': True, 'approximate': True, 'long': True, 'to': True, 'rgb': True, '2': True, 'easy': True, 'especially': True, 'was': True, 'more': True, 'then': True, 'it,': True, 'evening': True, 'that': True, 'very': True, 'Btw.': True, 'couple': True, 'Altough': True, 'space,': True, 'so,': True, 'cant': True, 'it': True, 'beers': True, 'difficult': True, 'not': True, \"I'm\": True, 'now': True, 'abandoned': True, 'me': True, 'easily': True, 'like': True, 'peak).': True, 'did': True, 'always': True, 'as': True, 'tracing,': True, 'wrong),': True, 'computations': True, 'of': True, '3d': True, 'even': True, 'something': True, 'and': True, 'because': True, 'do': True, 'physically': True, 'is': True, 'thus': True, 'spectrum': True, 'Long': True, 'outperforms': True, 'see': True, 'are': True, 'have': True, 'in': True, 'seen': True, 'implemented': True, 'linear': True, 'spectral': True, 'what': True, 'no': True, 'when': True, 'converge.': True, 'rewriting': True, 'does': True, 'yesterday': True, 'if': True, 'cause': True, 'correct': True, 'point?': True, 'poor': True, 'be': True, 'metropolis': True, 'after': True, 'involved.': True, 'interchangeably': True, 'I': True, 'caustics': True, 'that.': True, 'standard': True, 'applications': True, 'running': True, 'operating': True, 'MLT': True, 'fresh': True, 'path': True, 'a': True, 'on': True, 'benefits': True, '(please': True, 'java': True, 'nevertheless': True, 'tracing': True, 'uses': True, 'simulation': True, 'Ballmer': True, 'so': True, 'time': True, 'rgb.': True, 'the': True, 'or': True, 'realised': True}\n"
     ]
    }
   ],
   "source": [
    "def unigram_bow_rewrite(document):\n",
    "    return {token: True for token in document.split()}\n",
    "\n",
    "dataset_file = open('blog-gender-dataset.csv')\n",
    "raw_data = [(raw_text, normalize_label(label))  \n",
    "            for raw_text, label in csv.reader(dataset_file) if label] \n",
    "# lazy evaluation might be a better idea in real implementation\n",
    "            \n",
    "dataset_file.close()\n",
    "\n",
    "feature_functions = [unigram_bow_rewrite]\n",
    "print(extract_features(raw_data[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all together, we finally translate the entire dataset into `feature_representation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_representation = [(extract_features(raw_text), label) \n",
    "                          for raw_text, label in raw_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's split the dataset into the training set and the test set for testing the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset_percentage = 10\n",
    "\n",
    "def split_dataset(dataset, testset_percentage):\n",
    "    cutoff = testset_percentage * len(dataset) / 100\n",
    "    return dataset[cutoff:], dataset[:cutoff]\n",
    "\n",
    "trainset, testset = split_dataset(feature_representation, \n",
    "                                  testset_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifiers\n",
    "\n",
    "Once we have the feature representation, training a classifier is straightforward. We only need to call `train()` method on our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_classifier = nltk.classify.NaiveBayesClassifier.train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than Naive Bayes, NLTK comes with MaxEnt, decision tree, and many other classifier implementations. For details, refer to [the official documentation](http://www.nltk.org/api/nltk.classify.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a classifier\n",
    "\n",
    "Now let's see how good our naive bayes classifier is. After training, the classifier can perform classification upon `classify()` calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-778411f7b44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m predictions = [nb_classifier.classify(test_document) \n\u001b[0;32m----> 2\u001b[0;31m                for test_document, _ in testset]\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "predictions = [nb_classifier.classify(test_document) \n",
    "               for test_document, _ in testset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure its performance on many different aspect. First, let's see its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711180124224\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, golds):\n",
    "    return (len([p for p, g in zip(predictions, golds) if p == g]) \n",
    "            / float(len(golds)))\n",
    "\n",
    "golds = [label for _ , label in testset]\n",
    "print(accuracy(predictions, golds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute precision and recall, NLTK provides a convenient method to draw a confusion matrix as well as computation of precision and recall. However, we need a different data structure; sets of document id's of different labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   F   M |\n",
      "--+---------+\n",
      "F | <92> 36 |\n",
      "M |  57<137>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "<M> P: 0.79, R: 0.71, F: 0.75\n",
      "<F> P: 0.62, R: 0.72, F: 0.66\n"
     ]
    }
   ],
   "source": [
    "c_matrix = nltk.ConfusionMatrix(golds, predictions)\n",
    "print c_matrix\n",
    "\n",
    "gold_sets = {\"M\": set(), \"F\": set()}\n",
    "pred_sets = {\"M\": set(), \"F\": set()}\n",
    "for doc_id, label in enumerate(golds): \n",
    "    gold_sets[label].add(doc_id)\n",
    "for doc_id, label in enumerate(predictions): \n",
    "    pred_sets[label].add(doc_id)\n",
    "\n",
    "from nltk.metrics import scores\n",
    "for label in ['M', 'F']:\n",
    "    r = scores.recall(gold_sets[label], pred_sets[label])\n",
    "    p = scores.precision(gold_sets[label], pred_sets[label])\n",
    "    f = scores.f_measure(gold_sets[label], pred_sets[label])\n",
    "    print('<{}> P: {:.2}, R: {:.2}, F: {:.2}'.format(label, p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, but definately not least, NLTK provides a method to rank the helpfulness of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 devices = True                M : F      =     13.1 : 1.0\n",
      "                   users = True                M : F      =     12.8 : 1.0\n",
      "                  notion = True                M : F      =     12.4 : 1.0\n",
      "                    sun. = True                F : M      =     12.2 : 1.0\n",
      "                      ME = True                F : M      =     11.5 : 1.0\n",
      "                fabulous = True                F : M      =     10.2 : 1.0\n",
      "                  topped = True                F : M      =     10.1 : 1.0\n",
      "                   Bruce = True                M : F      =      9.9 : 1.0\n",
      "            technologies = True                M : F      =      9.9 : 1.0\n",
      "                    too! = True                F : M      =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have seen how to write Python code to transform documents into feature sets and to train/test a classifier for a simple classification problem. Now let's take a look at different types of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k_means clustering\n",
    "\n",
    "NLTK clustering implementations requires a **complete vector representation** (using `numpy.ndarray`) of the corpus, not a `dict` based featuresets.\n",
    "\n",
    "Let's do this with 128k unigram features, which will result in a very sparse feature vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "feature_indices = list(vocabulary) # we need an indexed feature names\n",
    "\n",
    "feature_vectors = []   # this will be a #documents X #features matrix\n",
    "                       # a huge and sparse one\n",
    "    \n",
    "def generate_document_vector(document):\n",
    "    document_vector = numpy.zeros(len(feature_indices))\n",
    "    for word_type in set(document.split()):\n",
    "        document_vector[feature_indices.index(word_type)] = 1\n",
    "    return document_vector\n",
    "\n",
    "feature_vectors = [generate_document_vector(document) \n",
    "                   for document, _ in raw_data[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, [`numpy.ndarray`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.ndarray.html) is a matrix-like python object that is highly optimized for numerical computation. Linear algebraic operations, such as arithmetic on matrices or matrix manipulations, are super fast with ndarrays, outperforming list comprehensions in Python with a huge gap (not to mention looping), so most scientific packages in Python including all widely-used machine learning packages (scikit-learn, theano, tensorflow, you name it) are heavily relying on `numpy` data structures and functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build 2 clusters using k-means algorithm based on cosine similarity between these vectors. Note that the k-means algorithm starts with random seeds and does not guarantee the global convergence. Thus, one might want to repeat the algorithm then take the most common result as a 'good enough' clustering (by using `repeats` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = nltk.cluster.cosine_distance\n",
    "kmc = nltk.cluster.kmeans.KMeansClusterer(2, dist, repeats=10)\n",
    "\n",
    "clustered = kmc.cluster(feature_vectors, True)\n",
    "print(clustered)\n",
    "\n",
    "gold_labels = [int(label=='M') for _, label in raw_data[:10]]\n",
    "print(gold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trained, the clustering can be used a sort-of classifier, like such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmc.classify(numpy.array(generate_document_vector(raw_data[111][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Sequencial Tagging\n",
    "\n",
    "### HMM tagger\n",
    "\n",
    "Here, we are going to train a HMM sequence tagger for Chinese word segmentation, using **BIO tagging**. Included in the directory is word segmentation annotation of around 20k Chinese sentences from news articles, a small part of [Chinese Treebank corpus](http://www.cs.brandeis.edu/~clp/ctb/). \n",
    "Let's take a look at the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最_B 近_O ，_B 我_B 们_O 通_B 过_O 知_B 情_O 人_B 士_O 从_B 衡_B 阳_I 市_O 殡_B 葬_O 管_B 理_I 处_O 财_B 务_O 部_B 门_O 复_B 印_O 出_B 部_B 分_O 原_B 始_O 发_B 票_O 凭_B 证_O 1_B 1_O 份_B （_B 共_B 计_O 有_B 3_B 0_I 余_O 份_B ）_B 。_B\n",
      "\n",
      "在_B 这_B 些_O 原_B 始_O 凭_B 证_O 上_B ，_B 我_B 们_O 可_B 以_O 清_B 清_I 楚_I 楚_O 地_B 看_B 到_O 如_B 此_O 的_B 批_B 示_O ：_B “_B 招_B 待_O 省_B 高_B 院_O 民_B 三_I 庭_O 法_B 官_O 来_B 衡_B 阳_O 调_B 查_O 取_B 证_O 餐_B 费_O ”_B 、_B “_B 到_B 省_B 高_B 院_O 协_B 调_O 与_B 明_B 田_O 公_B 司_O 诉_B 讼_O 问_B 题_O ，_B 招_B 待_O 上_B 级_O 领_B 导_O ”_B 、_B “_B 局_B 领_B 导_O 去_B 高_B 院_O ，_B 招_B 待_O 费_B 用_O ”_B 、_B “_B 招_B 待_O 省_B 高_B 院_O 领_B 导_O ”_B 、_B “_B 省_B 高_B 院_O 到_B 衡_B 阳_O 协_B 调_O 我_B 单_B 位_O 与_B 明_B 田_O 公_B 司_O 合_B 同_O 纠_B 纷_O 一_B 案_B ”_B 等_B 等_O （_B 证_B 据_O 复_B 印_I 件_O 附_B 后_B ）_B 。_B\n",
      "\n",
      "这_B 些_O 原_B 始_O 发_B 票_O 凭_B 证_O 足_B 以_O 证_B 明_O ，_B 这_B 是_B 一_B 起_B 典_B 型_O 的_B 司_B 法_O 不_B 公_O 、_B 司_B 法_O 腐_B 败_O 案_B ，_B 铁_B 证_I 如_I 山_O ！_B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('tagged.seg') as seg_annotation:\n",
    "    for i, line in enumerate(seg_annotation):\n",
    "        print line\n",
    "        if i > 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: For details for CTB, see the paper: \n",
    "* Xue, N., Xia, F., Chiou, F. D., & Palmer, M. (2005). The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural language engineering, 11(02), 207-238.\n",
    "\n",
    "The original word segmentation is done with simple whitespace, and I did BIO tagging based on the annotation last night. It was a complete hack job, and all errors in BIO tagging are my fault. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed the data into supervised training process implemented in NLTK, we need to prepare a sequence of <**observated, state**> tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19926\n",
      "[('\\xe5\\x8d\\x81', 'B'), ('\\xe4\\xb8\\x89', 'I'), ('\\xe4\\xba\\xbf', 'O'), ('\\xe6\\xb0\\x91', 'B'), ('\\xe8\\x8b\\xa6', 'B'), ('\\xe7\\x85\\x8e', 'B'), ('\\xe7\\x86\\xac', 'O')]\n"
     ]
    }
   ],
   "source": [
    "tagged_segmentation = []\n",
    "with open('tagged.seg') as annotation:\n",
    "    for line in annotation:\n",
    "        tagged_segmentation.append(\n",
    "            [tuple(token.split(\"_\")) for token in line.split()])\n",
    "    \n",
    "print(len(tagged_segmentation))\n",
    "print(tagged_segmentation[234])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have about 20k sequences of word segmentation tagging. As always, training starts with split the data into the train set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset, testset = split_dataset(tagged_segmentation, \n",
    "                                  testset_percentage)  # = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, training is fairly straightforward. We need to create an trainer object and then call `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "hmm_tagger = HiddenMarkovModelTrainer().train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all, we now have a tagger that can perform word segmentation on an arbitrary Chinese sentence. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori: 他们想着陶家会‘积极’赔偿履行诺言，陪同着孩子一直治疗直到好为止。\n",
      "gld: 他们 想 着 陶家 会 ‘ 积极 ’ 赔偿 履 行诺 言 ， 陪同 着 孩子 一直 治疗 直到 好 为止 。\n",
      "tag: 他们 想 着 陶家 会 ‘ 积极 ’ 赔偿 履 行诺 言 ， 陪同 着 孩子 一直 治疗 直到 好 为止 。\n"
     ]
    }
   ],
   "source": [
    "def glue_chars(chars_seq):\n",
    "    return \"\".join(chars_seq)\n",
    "\n",
    "def bio_to_whtspc(tagged_seq):\n",
    "    return glue_chars([\" \" + char if tag == \"B\" else char \n",
    "                       for char, tag in tagged]).strip()\n",
    "\n",
    "print \"ori:\", glue_chars([char for char, tag in testset[66]])\n",
    "print \"gld:\", bio_to_whtspc(testset[66])\n",
    "print \"tag:\", bio_to_whtspc(tagged_segmentation[66])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, how good does it perform in general? We can measure the tagger's accuracy with a simple computation, as we did in the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions= []\n",
    "gold = []\n",
    "\n",
    "for sent in testset:\n",
    "    predictions.extend([predicted for _, predicted \n",
    "                        in hmm_tagger.tag([char for char, tag in sent])])\n",
    "    gold.extend([gold_tag for _, gold_tag in sent])\n",
    "\n",
    "print \"accuracy:\", accuracy(predictions, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as confusion matrix, and P/R/F measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |     B     I     O |\n",
      "--+-------------------+\n",
      "B |<38122>  553  5850 |\n",
      "I |  1448  <527>  794 |\n",
      "O |  5253   199<14093>|\n",
      "--+-------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ConfusionMatrix(gold, predictions))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
